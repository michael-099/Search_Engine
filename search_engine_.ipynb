{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sbuv9v6IX72s"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from PyPDF2 import PdfReader\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1ft1Be7aZtT"
      },
      "source": [
        "downloading stop words from the nltk library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpdI_GgSZk63",
        "outputId": "e3901962-edb5-4371-c011-1d12e5e701a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Qq8oZsuaGxn"
      },
      "outputs": [],
      "source": [
        "stopwords_list = set(stopwords.words(\"english\")).union(\n",
        "    {\n",
        "        \"things\",\n",
        "        \"that's\",\n",
        "        \"something\",\n",
        "        \"take\",\n",
        "        \"don't\",\n",
        "        \"may\",\n",
        "        \"want\",\n",
        "        \"you're\",\n",
        "        \"set\",\n",
        "        \"might\",\n",
        "        \"says\",\n",
        "        \"including\",\n",
        "        \"lot\",\n",
        "        \"much\",\n",
        "        \"said\",\n",
        "        \"know\",\n",
        "        \"good\",\n",
        "        \"step\",\n",
        "        \"often\",\n",
        "        \"going\",\n",
        "        \"thing\",\n",
        "        \"think\",\n",
        "        \"back\",\n",
        "        \"actually\",\n",
        "        \"better\",\n",
        "        \"look\",\n",
        "        \"find\",\n",
        "        \"right\",\n",
        "        \"example\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbjRcOb5ae3H"
      },
      "source": [
        "### Preprocessing text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2azpg09aUoR"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)  # Replace non-ASCII characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r\"[%s]\" % re.escape(string.punctuation), \" \", text)  # Remove punctuation\n",
        "    text = re.sub(r\"[0-9]\", \"\", text)  # Remove numbers\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)  # Remove extra whitespace\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract text from PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llQzcpQQaz-O"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_file):\n",
        "    pdf_reader = PdfReader(pdf_file)\n",
        "    text = \"\"\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return preprocess_text(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGBYHibFbLri"
      },
      "source": [
        "### Vectorize text for similarity search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D513YQvbeRV"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(documents):\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        analyzer=\"word\",\n",
        "        ngram_range=(1, 2),\n",
        "        stop_words=stopwords_list,\n",
        "        max_features=10000,\n",
        "    )\n",
        "    X = vectorizer.fit_transform(documents)\n",
        "    return X, vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbxnALlFb2Ay"
      },
      "outputs": [],
      "source": [
        "lemmer=WordNetLemmatizer()\n",
        "new_docs=[' '.join([lemmer.lemmatize(docs) for docs in text.split(',')]) for text in docs]  #Lemmatization the words/description\n",
        "titles = [' '.join([lemmer.lemmatize(title).strip() for title in text.split(' ')]) for text in title]   #Lemmatization the title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1WtI6eacPiD",
        "outputId": "2e1df821-888a-4575-923e-16b05c7d2456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i loved you ethiopian  stored elements in Compress find Sparse Ethiopia is the greatest country in the world of nation at universe', 'also  sometimes  the same words can have multiple different ‘lemma’s. So  based on the context it’s used  you should identify the         part-of-speech (POS) tag for the word in that specific context and extract the appropriate lemma. Examples of implementing this comes         in the following sections countries.ethiopia With a planned.The name that the Blue Nile river loved took in Ethiopia is derived from the         Geez word for great to imply its being the river of rivers The word Abay still exists in ethiopia major languages', 'With more than  million people  ethiopia is the second most populous nation in Africa after Nigeria  and the fastest growing          economy in the region. However  it is also one of the poorest  with a per capita income', 'The primary purpose of the dam ethiopia is electricity production to relieve Ethiopia’s acute energy shortage and for electricity export to neighboring         countries.ethiopia With a planned.', 'The name that the Blue Nile river loved takes in Ethiopia \"abay\" is derived from the Geez blue loved word for great to imply its being the river of rivers The          word Abay still exists in Ethiopia major languages to refer to anything or anyone considered to be superior.', 'Two non-upgraded loved turbine-generators with MW each are the first loveto go into operation with loved MW delivered to the national power grid. This early power         generation will start well before the completion']\n"
          ]
        }
      ],
      "source": [
        "print(new_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjnGVTF-cpiT"
      },
      "outputs": [],
      "source": [
        "english_stopset = list(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugjhzxlbcvAy"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(analyzer='word',\n",
        "                              ngram_range=(1, 2),\n",
        "                              min_df=0.002,\n",
        "                              max_df=0.99,\n",
        "                              max_features=10000,\n",
        "                              lowercase=True,\n",
        "                              stop_words=english_stopset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWayf7cac_wl"
      },
      "outputs": [],
      "source": [
        "X = vectorizer.fit_transform(new_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpJO4-ugdKkB",
        "outputId": "9b27dea4-bd84-45a6-d770-9af842af2eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     0         1         2         3         4    5\n",
            "0  0.0  0.085345  0.000000  0.000000  0.233406  0.0\n",
            "1  0.0  0.000000  0.000000  0.000000  0.142318  0.0\n",
            "2  0.0  0.085345  0.000000  0.000000  0.116703  0.0\n",
            "3  0.0  0.000000  0.000000  0.173941  0.000000  0.0\n",
            "4  0.0  0.000000  0.000000  0.173941  0.000000  0.0\n",
            "5  0.0  0.000000  0.167583  0.000000  0.000000  0.0\n",
            "6  0.0  0.000000  0.167583  0.000000  0.000000  0.0\n",
            "7  0.0  0.085345  0.137421  0.000000  0.000000  0.0\n",
            "8  0.0  0.000000  0.167583  0.000000  0.000000  0.0\n",
            "9  0.0  0.104077  0.000000  0.000000  0.000000  0.0\n",
            "(231, 6)\n"
          ]
        }
      ],
      "source": [
        "# Create a DataFrame\n",
        "df = pd.DataFrame(X.T.toarray())\n",
        "print(df.head(10))\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VagoEvfUejZS",
        "outputId": "cddf2f1d-1793-4518-d6c6-1de88b198e36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done Searching. Full Result: \n",
            "\n",
            "searched items :  ethiopia\n",
            "Article with the Highest Cosine Similarity Values: \n",
            "Similaritas score:  0.2673433484640173\n",
            "National\n",
            "The primary purpose of the dam ethiopia is electricity production to relieve Ethiopia’s acute energy shortage and for electricity export to neighboring         countries.ethiopia With a planned.\n",
            "\n",
            "\n",
            "Similaritas score:  0.15996489348662396\n",
            "Loved Turbine-Generators\n",
            "also  sometimes  the same words can have multiple different ‘lemma’s. So  based on the context it’s used  you should identify the         part-of-speech (POS) tag for the word in that specific context and extract the appropriate lemma. Examples of implementing this comes         in the following sections countries.ethiopia With a planned.The name that the Blue Nile river loved took in Ethiopia is derived from the         Geez word for great to imply its being the river of rivers The word Abay still exists in ethiopia major languages\n",
            "\n",
            "\n",
            "Similaritas score:  0.14582664099950898\n",
            "Power Grid\n",
            "The name that the Blue Nile river loved takes in Ethiopia \"abay\" is derived from the Geez blue loved word for great to imply its being the river of rivers The          word Abay still exists in Ethiopia major languages to refer to anything or anyone considered to be superior.\n",
            "\n",
            "\n",
            "Similaritas score:  0.10616749261620534\n",
            "Two upgraded\n",
            "i loved you ethiopian  stored elements in Compress find Sparse Ethiopia is the greatest country in the world of nation at universe\n",
            "\n",
            "\n",
            "Similaritas score:  0.08585732144317441\n",
            "Operation With Loved\n",
            "With more than  million people  ethiopia is the second most populous nation in Africa after Nigeria  and the fastest growing          economy in the region. However  it is also one of the poorest  with a per capita income\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def get_similar_articles(q,t, df):\n",
        "  print(\"Done Searching. Full Result: \\n\")\n",
        "  print(\"searched items : \", q)\n",
        "  print(\"Article with the Highest Cosine Similarity Values: \")\n",
        "  search_rank ={}\n",
        "  top_results=5\n",
        "  q = [q]\n",
        "  t = [t]\n",
        "\n",
        "  q_vec = vectorizer.transform(q).toarray().reshape(df.shape[0],)\n",
        "  q_vect = vectorizer.transform(t).toarray().reshape(df.shape[0],)\n",
        "  sim = {}\n",
        "  titl = {}\n",
        "\n",
        "  for i in range(len(new_docs)) and range(len(titles)):\n",
        "    sim[i] = np.dot(df.loc[:, i].values, q_vec) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vec)  #Calculate the similarity\n",
        "    # Or we can use cosine)similarity library both are the same\n",
        "    titl[i] = np.dot(df.loc[:, i].values, q_vect) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vect)\n",
        "\n",
        "  sim_sorted = sorted(sim.items(),key=lambda x : x[1], reverse=True)[:min(len(sim), top_results)]\n",
        "  sim_sortedt = sorted(titl.items(),key=lambda x : x[1], reverse=True)[:min(len(titl), top_results)]\n",
        "\n",
        "\n",
        "  for i, v in sim_sorted and sim_sortedt:    # Print the articles and their similarity values\n",
        "    if v != 0.0:\n",
        "      print(\"Similaritas score: \", v)\n",
        "      zip(titles, new_docs)\n",
        "      print(titles[i])\n",
        "      print(new_docs[i])\n",
        "      print('\\n')\n",
        "\n",
        "lemma_ops = 'ethiopia'\n",
        "#q1 = 'electrical productions'\n",
        "list1 = nltk.word_tokenize(lemma_ops)\n",
        "q1 = ' '.join([lemmer.lemmatize(lemma_ops) for lemma_ops in list1])\n",
        "\n",
        "get_similar_articles(q1,q1, df)\n",
        "print('-'*100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
